{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_path = \"D:\\MSc Data Science\\Advanced Modules\\[INF-DSAM1B] Advanced Machine Learning B\\Deep learning for NLP\\Project\\Machine translation with attention\"\n",
    "english_data_path = \"Data\\\\es-en\"\n",
    "spanish_data_path = \"Data\\\\es-en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 'utf-8' removes b'' character string literal\n",
    "# splitlines() remove newline character\n",
    "with open(os.path.join(DIR_path, english_data_path, \"europarl-v7.es-en.en\"), \"rb\") as f:\n",
    "    content_english = f.read().decode(\"utf-8\").splitlines()\n",
    "\n",
    "#num_eng_words = 0\n",
    "#for i in content_english:\n",
    "#    num_eng_words += len(i.split(\" \"))\n",
    "#print(\"Number of english words: \", num_eng_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DIR_path, spanish_data_path, \"europarl-v7.es-en.es\"), \"rb\") as f:\n",
    "    content_spanish = f.read().decode(\"utf-8\").splitlines()\n",
    "\n",
    "#num_spn_words = 0\n",
    "#for i in content_spanish:\n",
    "#    num_spn_words += len(i.split(\" \"))\n",
    "#print(\"Number of spanish words: \", num_spn_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_preprocess(sentence):\n",
    "    sentence=sentence.lower()             \n",
    "    sentence = re.sub(r\"[-,.!?()]+\", r\"\", sentence)\n",
    "    return sentence\n",
    "\n",
    "_patterns = [r'\\'',\n",
    "             r'\\\"',\n",
    "             r'\\.',\n",
    "             r'<br \\/>',\n",
    "             r',',\n",
    "             r'\\(',\n",
    "             r'\\)',\n",
    "             r'\\!',\n",
    "             r'\\?',\n",
    "             r'\\;',\n",
    "             r'\\:',\n",
    "             r'\\s+']\n",
    "\n",
    "_replacements = [' \\'  ',\n",
    "                 '',\n",
    "                 ' . ',\n",
    "                 ' ',\n",
    "                 ' , ',\n",
    "                 ' ( ',\n",
    "                 ' ) ',\n",
    "                 ' ! ',\n",
    "                 ' ? ',\n",
    "                 ' ',\n",
    "                 ' ',\n",
    "                 ' ']\n",
    "\n",
    "_patterns_dict = list((re.compile(p), r) for p, r in zip(_patterns, _replacements))\n",
    "\n",
    "def sentence_preprocess(sentence):\n",
    "    \"\"\"https://pytorch.org/text/_modules/torchtext/data/utils.html\"\"\"\n",
    "    sentence = sentence.lower()\n",
    "    for pattern_re, replaced_str in _patterns_dict:\n",
    "        sentence = pattern_re.sub(replaced_str, sentence)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total english sentences:  1965734\n"
     ]
    }
   ],
   "source": [
    "# preprocess the english sentence\n",
    "sentence_english = []\n",
    "for sent in content_english:\n",
    "    sentence_english.append(sentence_preprocess(sent))\n",
    "print(\"total english sentences: \", len(sentence_english))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total spanish sentences:  1965734\n"
     ]
    }
   ],
   "source": [
    "# preprocess the spanish sentence\n",
    "sentence_spanish = []\n",
    "for sent in content_spanish:\n",
    "    sentence_spanish.append(sentence_preprocess(sent))\n",
    "print(\"total spanish sentences: \", len(sentence_spanish))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import nltk\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 3130.69it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop over wach of the sentence and tokenize eacch sentenec separately.\n",
    "# will take some time to tokenize each sentence.\n",
    "english_tokenized_text = [ nltk.word_tokenize(sentence_english[i], language=\"english\") for i in tqdm(range(len(sentence_english[:500]))) ]\n",
    "\n",
    "# create word index\n",
    "# assign each word a number.\n",
    "word_to_index = {}\n",
    "words=[]\n",
    "for sentence in english_tokenized_text:\n",
    "    for word in sentence:\n",
    "        words.append(word)\n",
    "UNIQUE_WORDS = set(words)\n",
    "\n",
    "for index, word in enumerate(UNIQUE_WORDS):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "# add tokens: <SOS> and <EOS>\n",
    "word_to_index[\"<SOS>\"] = list(word_to_index.values())[-1] + 1\n",
    "word_to_index[\"<EOS>\"] = list(word_to_index.values())[-1] + 1\n",
    "\n",
    "# using word index, create tensor\n",
    "# convert each of the sentence into numbers.\n",
    "english_tokenized_tensor = []\n",
    "\n",
    "for sentence in english_tokenized_text:\n",
    "    #english_tokenized_tensor.append( [word_to_index[word] for word in sentence]  )\n",
    "    tensor_list=[]\n",
    "    tensor_list.append(word_to_index[\"<SOS>\"])\n",
    "    tensor_list = tensor_list + [word_to_index[word] for word in sentence]\n",
    "    tensor_list.append(word_to_index[\"<EOS>\"])\n",
    "    english_tokenized_tensor.append(tensor_list)\n",
    "    \n",
    "data_english=[]\n",
    "for i in range(len(english_tokenized_text)):\n",
    "    data_english.append({\"TOKENIZED WORD\": english_tokenized_text[i] , \n",
    "             \"TENSOR\": torch.tensor(english_tokenized_tensor[i], dtype=torch.long), \n",
    "             \"LANGUAGE\": \"ENGLISH\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TOKENIZED WORD': ['resumption', 'of', 'the', 'session'],\n",
       "  'TENSOR': tensor([2348, 1510,  959,  971, 2224, 2349]),\n",
       "  'LANGUAGE': 'ENGLISH'},\n",
       " {'TOKENIZED WORD': ['i',\n",
       "   'declare',\n",
       "   'resumed',\n",
       "   'the',\n",
       "   'session',\n",
       "   'of',\n",
       "   'the',\n",
       "   'european',\n",
       "   'parliament',\n",
       "   'adjourned',\n",
       "   'on',\n",
       "   'friday',\n",
       "   '17',\n",
       "   'december',\n",
       "   '1999',\n",
       "   ',',\n",
       "   'and',\n",
       "   'i',\n",
       "   'would',\n",
       "   'like',\n",
       "   'once',\n",
       "   'again',\n",
       "   'to',\n",
       "   'wish',\n",
       "   'you',\n",
       "   'a',\n",
       "   'happy',\n",
       "   'new',\n",
       "   'year',\n",
       "   'in',\n",
       "   'the',\n",
       "   'hope',\n",
       "   'that',\n",
       "   'you',\n",
       "   'enjoyed',\n",
       "   'a',\n",
       "   'pleasant',\n",
       "   'festive',\n",
       "   'period',\n",
       "   '.'],\n",
       "  'TENSOR': tensor([2348,  424, 2152, 1245,  971, 2224,  959,  971, 1084,  754, 2125,  963,\n",
       "           943,  196,  402,   39, 1934, 1044,  424, 1227,  891,  854,  796, 1966,\n",
       "          1886,  847,  918, 1733,   93, 2187, 1148,  971, 1317, 2075,  847,  829,\n",
       "           918, 1606, 2214, 2082,  616, 2349]),\n",
       "  'LANGUAGE': 'ENGLISH'},\n",
       " {'TOKENIZED WORD': ['although',\n",
       "   ',',\n",
       "   'as',\n",
       "   'you',\n",
       "   'will',\n",
       "   'have',\n",
       "   'seen',\n",
       "   ',',\n",
       "   'the',\n",
       "   'dreaded',\n",
       "   \"'\",\n",
       "   'millennium',\n",
       "   'bug',\n",
       "   \"'\",\n",
       "   'failed',\n",
       "   'to',\n",
       "   'materialise',\n",
       "   ',',\n",
       "   'still',\n",
       "   'the',\n",
       "   'people',\n",
       "   'in',\n",
       "   'a',\n",
       "   'number',\n",
       "   'of',\n",
       "   'countries',\n",
       "   'suffered',\n",
       "   'a',\n",
       "   'series',\n",
       "   'of',\n",
       "   'natural',\n",
       "   'disasters',\n",
       "   'that',\n",
       "   'truly',\n",
       "   'were',\n",
       "   'dreadful',\n",
       "   '.'],\n",
       "  'TENSOR': tensor([2348,  430, 1934,   72,  847, 1207, 1620,  615, 1934,  971,  179,  791,\n",
       "          2086, 1946,  791, 1149, 1966, 1786, 1934, 1059,  971, 2124, 1148,  918,\n",
       "           924,  959, 1295, 2222,  918, 1710,  959,  177, 1147, 2075,  265, 1768,\n",
       "          1370,  616, 2349]),\n",
       "  'LANGUAGE': 'ENGLISH'}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# this is how we access the data\n",
    "###data_english[0][\"TENSOR\"]\n",
    "data_english[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 500/500 [00:00<00:00, 2653.20it/s]\n"
     ]
    }
   ],
   "source": [
    "# Loop over wach of the sentence and tokenize eacch sentenec separately.\n",
    "# will take some time to tokenize each sentence.\n",
    "spanish_tokenized_text = [ nltk.word_tokenize(sentence_spanish[i], language=\"spanish\") for i in tqdm(range(len(sentence_spanish[:500]))) ]\n",
    "\n",
    "# create word index\n",
    "# assign each word a number.\n",
    "word_to_index = {}\n",
    "words=[]\n",
    "for sentence in spanish_tokenized_text:\n",
    "    for word in sentence:\n",
    "        words.append(word)\n",
    "UNIQUE_WORDS = set(words)\n",
    "\n",
    "for index, word in enumerate(UNIQUE_WORDS):\n",
    "    word_to_index[word] = index\n",
    "\n",
    "# add tokens: <SOS> and <EOS>\n",
    "word_to_index[\"<SOS>\"] = list(word_to_index.values())[-1] + 1\n",
    "word_to_index[\"<EOS>\"] = list(word_to_index.values())[-1] + 1\n",
    "\n",
    "# using word index, create tensor\n",
    "# convert each of the sentence into numbers.\n",
    "spanish_tokenized_tensor = []\n",
    "\n",
    "for sentence in spanish_tokenized_text:\n",
    "    \n",
    "    tensor_list=[]\n",
    "    tensor_list.append(word_to_index[\"<SOS>\"])\n",
    "    tensor_list = tensor_list + [word_to_index[word] for word in sentence]\n",
    "    tensor_list.append(word_to_index[\"<EOS>\"])\n",
    "    spanish_tokenized_tensor.append(tensor_list)\n",
    "    \n",
    "data_spanish=[]\n",
    "for i in range(len(spanish_tokenized_text)):\n",
    "    data_spanish.append({\"TOKENIZED WORD\": spanish_tokenized_text[i] , \n",
    "             \"TENSOR\": torch.tensor(spanish_tokenized_tensor[i], dtype=torch.long), \n",
    "             \"LANGUAGE\": \"ENGLISH\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'TOKENIZED WORD': ['reanudación', 'del', 'período', 'de', 'sesiones'],\n",
       "  'TENSOR': tensor([2854, 2018, 1057,  915,  909,  307, 2855]),\n",
       "  'LANGUAGE': 'ENGLISH'},\n",
       " {'TOKENIZED WORD': ['declaro',\n",
       "   'reanudado',\n",
       "   'el',\n",
       "   'período',\n",
       "   'de',\n",
       "   'sesiones',\n",
       "   'del',\n",
       "   'parlamento',\n",
       "   'europeo',\n",
       "   ',',\n",
       "   'interrumpido',\n",
       "   'el',\n",
       "   'viernes',\n",
       "   '17',\n",
       "   'de',\n",
       "   'diciembre',\n",
       "   'pasado',\n",
       "   ',',\n",
       "   'y',\n",
       "   'reitero',\n",
       "   'a',\n",
       "   'sus',\n",
       "   'señorías',\n",
       "   'mi',\n",
       "   'deseo',\n",
       "   'de',\n",
       "   'que',\n",
       "   'hayan',\n",
       "   'tenido',\n",
       "   'unas',\n",
       "   'buenas',\n",
       "   'vacaciones',\n",
       "   '.'],\n",
       "  'TENSOR': tensor([2854, 1722, 2624, 1694,  915,  909,  307, 1057, 2756, 1499, 2369,   27,\n",
       "          1694,  552,  223,  909, 1663,  234, 2369, 1426,  141, 1133,  895, 2170,\n",
       "          2647, 2260,  909, 2785,  209,  577, 1044, 1819, 2819,  775, 2855]),\n",
       "  'LANGUAGE': 'ENGLISH'},\n",
       " {'TOKENIZED WORD': ['como',\n",
       "   'todos',\n",
       "   'han',\n",
       "   'podido',\n",
       "   'comprobar',\n",
       "   ',',\n",
       "   'el',\n",
       "   'gran',\n",
       "   'efecto',\n",
       "   'del',\n",
       "   'año',\n",
       "   '2000',\n",
       "   'no',\n",
       "   'se',\n",
       "   'ha',\n",
       "   'producido',\n",
       "   '.',\n",
       "   'en',\n",
       "   'cambio',\n",
       "   ',',\n",
       "   'los',\n",
       "   'ciudadanos',\n",
       "   'de',\n",
       "   'varios',\n",
       "   'de',\n",
       "   'nuestros',\n",
       "   'países',\n",
       "   'han',\n",
       "   'sido',\n",
       "   'víctimas',\n",
       "   'de',\n",
       "   'catástrofes',\n",
       "   'naturales',\n",
       "   'verdaderamente',\n",
       "   'terribles',\n",
       "   '.'],\n",
       "  'TENSOR': tensor([2854,  295, 2701,  122, 1796,  322, 2369, 1694, 2430,  978, 1057,  634,\n",
       "          1826,  983,  246, 2032, 1274,  775, 2233, 1652, 2369, 2332, 1879,  909,\n",
       "          1897,  909, 1227, 2817,  122,  367, 2262,  909, 2407,  720, 2420, 2352,\n",
       "           775, 2855]),\n",
       "  'LANGUAGE': 'ENGLISH'}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_spanish[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
